Include full register state in ARM Cortex M Exception Stack Frame (ESF)

From: Memfault Inc <hello@memfault.com>

[PATCH] Extend Zephyr Fault Handler to capture full register state as part of
the fault handling process. This enables local variable state and a
full backtrace leading up to the crash to be recovered for post-mortem
debugging.
---
 arch/arm/core/aarch32/cortex_m/fault.c   |   80 ++++++++++++++----------------
 arch/arm/core/aarch32/cortex_m/fault_s.S |   49 ++++++++++++++++--
 arch/arm/core/aarch32/fatal.c            |   43 ++++++++++------
 arch/arm/include/aarch32/cortex_m/exc.h  |    2 -
 include/arch/arm/aarch32/exc.h           |   22 ++++++++
 5 files changed, 127 insertions(+), 69 deletions(-)

diff --git a/arch/arm/core/aarch32/cortex_m/fault.c b/arch/arm/core/aarch32/cortex_m/fault.c
index 38748f387d..a74e691563 100644
--- a/arch/arm/core/aarch32/cortex_m/fault.c
+++ b/arch/arm/core/aarch32/cortex_m/fault.c
@@ -173,13 +173,14 @@ static const struct z_exc_handle exceptions[] = {
 static bool memory_fault_recoverable(z_arch_esf_t *esf)
 {
 #ifdef CONFIG_USERSPACE
+	struct __esf *hw_esf = esf->exception_frame;
 	for (int i = 0; i < ARRAY_SIZE(exceptions); i++) {
 		/* Mask out instruction mode */
 		u32_t start = (u32_t)exceptions[i].start & ~0x1;
 		u32_t end = (u32_t)exceptions[i].end & ~0x1;
 
-		if (esf->basic.pc >= start && esf->basic.pc < end) {
-			esf->basic.pc = (u32_t)(exceptions[i].fixup);
+		if (hw_esf->basic.pc >= start && hw_esf->basic.pc < end) {
+			hw_esf->basic.pc = (u32_t)(exceptions[i].fixup);
 			return true;
 		}
 	}
@@ -613,7 +614,7 @@ static u32_t hard_fault(z_arch_esf_t *esf, bool *recoverable)
 	 * priority. We handle the case of Kernel OOPS and Stack
 	 * Fail here.
 	 */
-	u16_t *ret_addr = (u16_t *)esf->basic.pc;
+	u16_t *ret_addr = (u16_t *)esf->exception_frame->basic.pc;
 	/* SVC is a 16-bit instruction. On a synchronous SVC
 	 * escalated to Hard Fault, the return address is the
 	 * next instruction, i.e. after the SVC.
@@ -624,8 +625,8 @@ static u32_t hard_fault(z_arch_esf_t *esf, bool *recoverable)
 	if (((fault_insn & 0xff00) == _SVC_OPCODE) &&
 		((fault_insn & 0x00ff) == _SVC_CALL_RUNTIME_EXCEPT)) {
 
-		PR_EXC("ARCH_EXCEPT with reason %x\n", esf->basic.r0);
-		reason = esf->basic.r0;
+		reason = esf->exception_frame->basic.r0;
+		PR_EXC("ARCH_EXCEPT with reason %x\n", reason);
 	}
 #undef _SVC_OPCODE
 
@@ -728,7 +729,7 @@ static u32_t fault_handle(z_arch_esf_t *esf, int fault, bool *recoverable)
  *
  * @param secure_esf Pointer to the secure stack frame.
  */
-static void secure_stack_dump(const z_arch_esf_t *secure_esf)
+static void secure_stack_dump(const struct __esf *secure_esf)
 {
 	/*
 	 * In case a Non-Secure exception interrupted the Secure
@@ -753,7 +754,7 @@ static void secure_stack_dump(const z_arch_esf_t *secure_esf)
 		 * Non-Secure exception entry.
 		 */
 		top_of_sec_stack += ADDITIONAL_STATE_CONTEXT_WORDS;
-		secure_esf = (const z_arch_esf_t *)top_of_sec_stack;
+		secure_esf = (const struct __esf *)top_of_sec_stack;
 		sec_ret_addr = secure_esf->basic.pc;
 	} else {
 		/* Exception during Non-Secure function call.
@@ -782,11 +783,11 @@ static void secure_stack_dump(const z_arch_esf_t *secure_esf)
  *
  * @return ESF pointer on success, otherwise return NULL
  */
-static inline z_arch_esf_t *get_esf(u32_t msp, u32_t psp, u32_t exc_return,
+static inline void *get_hardware_esf(u32_t msp, u32_t psp, u32_t exc_return,
 	bool *nested_exc)
 {
 	bool alternative_state_exc = false;
-	z_arch_esf_t *ptr_esf;
+	u32_t ptr_esf;
 
 	*nested_exc = false;
 
@@ -814,27 +815,27 @@ static inline z_arch_esf_t *get_esf(u32_t msp, u32_t psp, u32_t exc_return,
 		alternative_state_exc = true;
 
 		/* Dump the Secure stack before handling the actual fault. */
-		z_arch_esf_t *secure_esf;
+		u32_t secure_esf;
 
 		if (exc_return & EXC_RETURN_SPSEL_PROCESS) {
 			/* Secure stack pointed by PSP */
-			secure_esf = (z_arch_esf_t *)psp;
+			secure_esf = psp;
 		} else {
 			/* Secure stack pointed by MSP */
-			secure_esf = (z_arch_esf_t *)msp;
+			secure_esf = msp;
 			*nested_exc = true;
 		}
 
-		SECURE_STACK_DUMP(secure_esf);
+		SECURE_STACK_DUMP((struct __esf *)secure_esf);
 
 		/* Handle the actual fault.
 		 * Extract the correct stack frame from the Non-Secure state
 		 * and supply it to the fault handing function.
 		 */
 		if (exc_return & EXC_RETURN_MODE_THREAD) {
-			ptr_esf = (z_arch_esf_t *)__TZ_get_PSP_NS();
+			ptr_esf = __TZ_get_PSP_NS();
 		} else {
-			ptr_esf = (z_arch_esf_t *)__TZ_get_MSP_NS();
+			ptr_esf = __TZ_get_MSP_NS();
 		}
 	}
 #elif defined(CONFIG_ARM_NONSECURE_FIRMWARE)
@@ -859,10 +860,10 @@ static inline z_arch_esf_t *get_esf(u32_t msp, u32_t psp, u32_t exc_return,
 
 		if (exc_return & EXC_RETURN_SPSEL_PROCESS) {
 			/* Non-Secure stack frame on PSP */
-			ptr_esf = (z_arch_esf_t *)psp;
+			ptr_esf = psp;
 		} else {
 			/* Non-Secure stack frame on MSP */
-			ptr_esf = (z_arch_esf_t *)msp;
+			ptr_esf = msp;
 		}
 	} else {
 		/* Exception entry occurred in Non-Secure stack. */
@@ -881,16 +882,16 @@ static inline z_arch_esf_t *get_esf(u32_t msp, u32_t psp, u32_t exc_return,
 	if (!alternative_state_exc) {
 		if (exc_return & EXC_RETURN_MODE_THREAD) {
 			/* Returning to thread mode */
-			ptr_esf =  (z_arch_esf_t *)psp;
+			ptr_esf =  psp;
 
 		} else {
 			/* Returning to handler mode */
-			ptr_esf = (z_arch_esf_t *)msp;
+			ptr_esf = msp;
 			*nested_exc = true;
 		}
 	}
 
-	return ptr_esf;
+	return (void *)ptr_esf;
 }
 
 /**
@@ -920,20 +921,22 @@ static inline z_arch_esf_t *get_esf(u32_t msp, u32_t psp, u32_t exc_return,
  *
  * @param msp MSP value immediately after the exception occurred
  * @param psp PSP value immediately after the exception occurred
- * @param exc_return EXC_RETURN value present in LR after exception entry.
+ * @param callee_regs Registers that are not part of the hardware esf
+ * (r4-r11 & EXC_RETURN)
  *
  */
-void z_arm_fault(u32_t msp, u32_t psp, u32_t exc_return)
+void z_arm_fault(u32_t msp, u32_t psp, struct __callee_saved_esf *callee_regs)
 {
 	u32_t reason = K_ERR_CPU_EXCEPTION;
 	int fault = SCB->ICSR & SCB_ICSR_VECTACTIVE_Msk;
 	bool recoverable, nested_exc;
-	z_arch_esf_t *esf;
+	struct __esf *hardware_esf;
+	const u32_t exc_return = callee_regs->exc_return;
 
 	/* Create a stack-ed copy of the ESF to be used during
 	 * the fault handling process.
 	 */
-	z_arch_esf_t esf_copy;
+	z_arch_esf_t esf;
 
 	/* Force unlock interrupts */
 	arch_irq_unlock(0);
@@ -941,32 +944,21 @@ void z_arm_fault(u32_t msp, u32_t psp, u32_t exc_return)
 	/* Retrieve the Exception Stack Frame (ESF) to be supplied
 	 * as argument to the remainder of the fault handling process.
 	 */
-	 esf = get_esf(msp, psp, exc_return, &nested_exc);
-	__ASSERT(esf != NULL,
+
+	 hardware_esf = get_hardware_esf(msp, psp, exc_return, &nested_exc);
+	__ASSERT(hardware_esf != NULL,
 		"ESF could not be retrieved successfully. Shall never occur.");
 
-	reason = fault_handle(esf, fault, &recoverable);
+	esf.exception_frame = hardware_esf;
+	esf.nested_exc = nested_exc;
+	esf.callee_regs = callee_regs;
+
+	reason = fault_handle(&esf, fault, &recoverable);
 	if (recoverable) {
 		return;
 	}
 
-	/* Copy ESF */
-	memcpy(&esf_copy, esf, sizeof(z_arch_esf_t));
-
-	/* Overwrite stacked IPSR to mark a nested exception,
-	 * or a return to Thread mode. Note that this may be
-	 * required, if the retrieved ESF contents are invalid
-	 * due to, for instance, a stacking error.
-	 */
-	if (nested_exc) {
-		if ((esf_copy.basic.xpsr & IPSR_ISR_Msk) == 0) {
-			esf_copy.basic.xpsr |= IPSR_ISR_Msk;
-		}
-	} else {
-		esf_copy.basic.xpsr &= ~(IPSR_ISR_Msk);
-	}
-
-	z_arm_fatal_error(reason, &esf_copy);
+	z_arm_fatal_error(reason, &esf);
 }
 
 /**
diff --git a/arch/arm/core/aarch32/cortex_m/fault_s.S b/arch/arm/core/aarch32/cortex_m/fault_s.S
index b691807ad1..b0895dacce 100644
--- a/arch/arm/core/aarch32/cortex_m/fault_s.S
+++ b/arch/arm/core/aarch32/cortex_m/fault_s.S
@@ -45,10 +45,10 @@ GTEXT(z_arm_exc_spurious)
  * The function supplies the values of
  * - the MSP
  * - the PSP
- * - the EXC_RETURN value
+ * - Register state not preserved by hardware (r4-r11 & EXC_RETURN)
  * as parameters to the z_arm_fault() C function that will perform the
- * rest of the fault handling (i.e. z_arm_fault(MSP, PSP, EXC_RETURN)).
-
+ * rest of the fault handling:
+ *    (i.e. z_arm_fault(MSP, PSP, struct __callee_saved_esf *)).
  * Provides these symbols:
  *
  *   z_arm_hard_fault
@@ -76,14 +76,49 @@ SECTION_SUBSEC_FUNC(TEXT,__fault,z_arm_debug_monitor)
 #endif /* CONFIG_ARMV6_M_ARMV8_M_BASELINE */
 SECTION_SUBSEC_FUNC(TEXT,__fault,z_arm_exc_spurious)
 
+#if defined(CONFIG_ARMV6_M_ARMV8_M_BASELINE)
+	/* Build __callee_saved_esf */
+	mov r3, r11
+	mov r2, r10
+	mov r1, r9
+	mov r0, r8
+	push {r0-r3, lr}
+	push {r3-r7}
+
+	/* Recover MSP at exception entry. */
 	mrs r0, MSP
+	adds r0, r0, #40
+
 	mrs r1, PSP
-	mov r2, lr /* EXC_RETURN */
+#else
+	mrs r0, MSP
+	mrs r1, PSP
+	/* Build __callee_saved_esf */
+	push {r3-r11, lr}
+#endif
 
-	push {r0, lr}
+	/**
+	* Recover pointer to __callee_saved_esf and pass set it as our
+	* 3rd argument to z_arm_fault(). Note we pushed one extra register, r3,
+	* to double-word align the stack to satisfy ABI requirements so we
+	* need to add 4 to the current sp to get the struct start address.
+	*/
+	mov r2, sp
+	adds r2, r2, #4
 
 	bl z_arm_fault
 
-	pop {r0, pc}
 
-	.end
+#if defined(CONFIG_ARMV6_M_ARMV8_M_BASELINE)
+	pop { r3-r7 }
+	/* restore r8-r11 */
+	pop { r0-r3 }
+	mov r11, r3
+	mov r10, r2
+	mov r9, r1
+	mov r8, r0
+	pop { pc }
+#else
+	pop {r3-r11, pc}
+#endif
+        .end
diff --git a/arch/arm/core/aarch32/fatal.c b/arch/arm/core/aarch32/fatal.c
index 849a4ee69c..f49a5926c3 100644
--- a/arch/arm/core/aarch32/fatal.c
+++ b/arch/arm/core/aarch32/fatal.c
@@ -18,24 +18,26 @@ LOG_MODULE_DECLARE(os);
 
 static void esf_dump(const z_arch_esf_t *esf)
 {
-	LOG_ERR("r0/a1:  0x%08x  r1/a2:  0x%08x  r2/a3:  0x%08x",
-		esf->basic.a1, esf->basic.a2, esf->basic.a3);
-	LOG_ERR("r3/a4:  0x%08x r12/ip:  0x%08x r14/lr:  0x%08x",
-		esf->basic.a4, esf->basic.ip, esf->basic.lr);
-	LOG_ERR(" xpsr:  0x%08x", esf->basic.xpsr);
+	const struct __esf *hw_esf = esf->exception_frame;
+
+	LOG_ERR("r0/a1:	 0x%08x	 r1/a2:	 0x%08x	 r2/a3:	 0x%08x",
+		hw_esf->basic.a1, hw_esf->basic.a2, hw_esf->basic.a3);
+	LOG_ERR("r3/a4:	 0x%08x r12/ip:	 0x%08x r14/lr:	 0x%08x",
+		hw_esf->basic.a4, hw_esf->basic.ip, hw_esf->basic.lr);
+	LOG_ERR(" xpsr:	 0x%08x", hw_esf->basic.xpsr);
 #if defined(CONFIG_FPU) && defined(CONFIG_FPU_SHARING)
 	for (int i = 0; i < 16; i += 4) {
 		LOG_ERR("s[%2d]:  0x%08x  s[%2d]:  0x%08x"
 			"  s[%2d]:  0x%08x  s[%2d]:  0x%08x",
-			i, (u32_t)esf->s[i],
-			i + 1, (u32_t)esf->s[i + 1],
-			i + 2, (u32_t)esf->s[i + 2],
-			i + 3, (u32_t)esf->s[i + 3]);
+			i, (u32_t)hw_esf->s[i],
+			i + 1, (u32_t)hw_esf->s[i + 1],
+			i + 2, (u32_t)hw_esf->s[i + 2],
+			i + 3, (u32_t)hw_esf->s[i + 3]);
 	}
-	LOG_ERR("fpscr:  0x%08x", esf->fpscr);
+	LOG_ERR("fpscr:	 0x%08x", hw_esf->fpscr);
 #endif
 	LOG_ERR("Faulting instruction address (r15/pc): 0x%08x",
-		esf->basic.pc);
+		hw_esf->basic.pc);
 }
 
 void z_arm_fatal_error(unsigned int reason, const z_arch_esf_t *esf)
@@ -65,7 +67,7 @@ void z_arm_fatal_error(unsigned int reason, const z_arch_esf_t *esf)
 void z_do_kernel_oops(const z_arch_esf_t *esf)
 {
 	/* Stacked R0 holds the exception reason. */
-	unsigned int reason = esf->basic.r0;
+	unsigned int reason = esf->exception_frame->basic.r0;
 
 #if defined(CONFIG_USERSPACE)
 	if ((__get_CONTROL() & CONTROL_nPRIV_Msk) == CONTROL_nPRIV_Msk) {
@@ -75,8 +77,10 @@ void z_do_kernel_oops(const z_arch_esf_t *esf)
 		 * User mode is only allowed to induce oopses and stack check
 		 * failures via software-triggered system fatal exceptions.
 		 */
-		if (!((esf->basic.r0 == K_ERR_KERNEL_OOPS) ||
-			(esf->basic.r0 == K_ERR_STACK_CHK_FAIL))) {
+		const struct __esf *hw_esf = esf->exception_frame;
+
+		if (!((hw_esf->basic.r0 == K_ERR_KERNEL_OOPS) ||
+			(hw_esf->basic.r0 == K_ERR_STACK_CHK_FAIL))) {
 
 			reason = K_ERR_KERNEL_OOPS;
 		}
@@ -89,10 +93,17 @@ void z_do_kernel_oops(const z_arch_esf_t *esf)
 FUNC_NORETURN void arch_syscall_oops(void *ssf_ptr)
 {
 	u32_t *ssf_contents = ssf_ptr;
-	z_arch_esf_t oops_esf = { 0 };
 
 	/* TODO: Copy the rest of the register set out of ssf_ptr */
-	oops_esf.basic.pc = ssf_contents[3];
+	struct __esf hw_esf = {
+	  .basic.pc =  ssf_contents[3],
+	};
+	struct __callee_saved_esf callee_regs = { 0 };
+
+	z_arch_esf_t oops_esf = {
+	  .exception_frame = &hw_esf,
+	  .callee_regs = &callee_regs,
+	};
 
 	z_arm_fatal_error(K_ERR_KERNEL_OOPS, &oops_esf);
 	CODE_UNREACHABLE;
diff --git a/arch/arm/include/aarch32/cortex_m/exc.h b/arch/arm/include/aarch32/cortex_m/exc.h
index 05ba19b0c5..40cfa887cd 100644
--- a/arch/arm/include/aarch32/cortex_m/exc.h
+++ b/arch/arm/include/aarch32/cortex_m/exc.h
@@ -70,7 +70,7 @@ static ALWAYS_INLINE bool arch_is_in_isr(void)
  */
 static ALWAYS_INLINE bool arch_is_in_nested_exception(const z_arch_esf_t *esf)
 {
-	return (esf->basic.xpsr & IPSR_ISR_Msk) ? (true) : (false);
+	return esf->nested_exc;
 }
 
 /**
diff --git a/include/arch/arm/aarch32/exc.h b/include/arch/arm/aarch32/exc.h
index 75050d17f1..efd8912633 100644
--- a/include/arch/arm/aarch32/exc.h
+++ b/include/arch/arm/aarch32/exc.h
@@ -73,6 +73,7 @@ GTEXT(z_arm_exc_exit);
 extern "C" {
 #endif
 
+/* The stack frame pushed by the MCU on exception entry */
 struct __esf {
 	struct __basic_sf {
 		sys_define_gpr_with_alias(a1, r0);
@@ -91,7 +92,26 @@ struct __esf {
 #endif
 };
 
-typedef struct __esf z_arch_esf_t;
+/* The registers not preserved by hardware on exception entry */
+struct __callee_saved_esf {
+	u32_t r4;
+	u32_t r5;
+	u32_t r6;
+	u32_t r7;
+	u32_t r8;
+	u32_t r9;
+	u32_t r10;
+	u32_t r11;
+	u32_t exc_return;
+};
+
+struct __full_esf {
+	struct __esf *exception_frame;
+	bool nested_exc;
+	struct __callee_saved_esf *callee_regs;
+};
+
+typedef struct __full_esf z_arch_esf_t;
 
 #ifdef CONFIG_CPU_CORTEX_M
 extern void z_arm_exc_exit(void);
